{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fasttext word2vec"
      ],
      "metadata": {
        "id": "0ZI1iy1xh3Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### in this notebook we train fasttext and word2vec models with gnesim and plot commons persian words."
      ],
      "metadata": {
        "id": "IudijWemhmnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFwAO9amVSOd"
      },
      "outputs": [],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n"
      ],
      "metadata": {
        "id": "qtkcqr1dVV2N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# read Documents from json file\n",
        "dataset = pd.read_json('/content/drive/MyDrive/datasets/data.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azZVPXw9VxVP",
        "outputId": "3d8f84a8-ce38-4ae8-ee70-dfa28c85cb3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocessing_Calss\n",
        "import hazm\n",
        "import string\n",
        "import re\n",
        "\n",
        "class preprocessing:\n",
        "  def __init__(self):\n",
        "    persian_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
        "    self.punctuations_list = string.punctuation + persian_punctuations\n",
        "    self.arabic_diacritics = re.compile(\"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    self.stop_words = hazm.stopwords_list()\n",
        "    self.lemmatizer = hazm.Lemmatizer()\n",
        "\n",
        "  def fit(self, train_data):\n",
        "    train_data['text'] = train_data['text'].apply(self._remove_diacritics)\n",
        "    train_data['text'] = train_data['text'].apply(self._remove_punctuations)\n",
        "    train_data['text'] = train_data['text'].apply(self._remove_repeating_char)\n",
        "    train_data['text'] = train_data['text'].apply(self._normalize_persian)\n",
        "    train_data['text'] = train_data['text'].apply(self._tokenize)\n",
        "    train_data['text'] = train_data['text'].apply(self._remove_stopwords)\n",
        "    train_data['text'] = train_data['text'].apply(self._lemmatizer)\n",
        "    return train_data\n",
        "\n",
        "\n",
        "  def _remove_diacritics(self, text):\n",
        "    text = re.sub(self.arabic_diacritics, '', text)\n",
        "    return text\n",
        "\n",
        "  def _remove_punctuations(self, text):\n",
        "    translator = str.maketrans('', '', self.punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "  def _remove_repeating_char(self, text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "\n",
        "\n",
        "  def _normalize_persian(self, text):\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ي\", \"ی\", text)\n",
        "    text = re.sub(\"ؤ\", \"و\", text)\n",
        "    text = re.sub(\"ئ\", \"ی\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"ك\" ,\"ک\" , text)\n",
        "    text = re.sub(\"[^ابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی]\", \" \", text)\n",
        "    text = re.sub(\"[^\\S\\n\\t]+\", ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "  def _tokenize(self, text):\n",
        "    return text.split()\n",
        "\n",
        "  def _remove_stopwords(self, words):\n",
        "    return [word  for word in words if word not in self.stop_words]\n",
        "\n",
        "  def _lemmatizer(self, words):\n",
        "    result = set()\n",
        "    for token in words:\n",
        "      result.add(self.lemmatizer.lemmatize(token))\n",
        "    return list(result)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mHJflb8vV41J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = preprocessing()\n",
        "data = pp.fit(dataset)"
      ],
      "metadata": {
        "id": "elkXO6ZpV-22"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Word2vec and Fasttext models"
      ],
      "metadata": {
        "id": "7A0OVnzBgeCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to train Word2Vec model\n",
        "def train_word2vec(data):\n",
        "    model = Word2Vec(data['text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PGr0yva3WIOl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train FastText model\n",
        "def train_fasttext(data):\n",
        "    model = FastText(data['text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KjttIzQrWaAo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting common persian words"
      ],
      "metadata": {
        "id": "OBtY-50ygqZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot embeddings\n",
        "def plot_embeddings(model, name):\n",
        "\n",
        "    # read the words stored in persian_words.txt\n",
        "    words_path = '/content/drive/MyDrive/datasets/827_common_parsi_words.txt'\n",
        "    with open(words_path, 'r') as f:\n",
        "        words = f.readlines()\n",
        "    words = [w.strip() for w in words]\n",
        "\n",
        "    # get the text embeddings for the words in the list of words\n",
        "    vecs = []\n",
        "    out_of_vocabulary = 0\n",
        "\n",
        "    for w in words:\n",
        "        try:\n",
        "            vecs.append(model.wv[w])\n",
        "        except Exception as e:\n",
        "            out_of_vocabulary += 1\n",
        "\n",
        "    print(\"out_of_vocabulary:\",out_of_vocabulary)\n",
        "    vecs = np.array(vecs)\n",
        "\n",
        "    # apply dimensionality reduction to the vector representations using tSNE method\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=20000)\n",
        "    results = tsne.fit_transform(vecs)\n",
        "    # plot the results with plotly package\n",
        "    plots = []\n",
        "    for i in range(len(words) - out_of_vocabulary):\n",
        "        pl = go.Scatter(x=[results[i, 0]], y=[results[i, 1]], mode='markers+text',text=[words[i]],\n",
        "                        textposition='bottom center',marker=dict(size=10, color=i, colorscale='Jet', opacity=0.8),\n",
        "                        textfont=dict(size=14,),name=words[i])\n",
        "        plots.append(pl)\n",
        "\n",
        "    py.plot(plots, filename= name +'tsne_persianwords.html', auto_open=True)\n"
      ],
      "metadata": {
        "id": "OOMKmBfJWftt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train models\n",
        "word2vec_model = train_word2vec(data)\n",
        "fasttext_model = train_fasttext(data)\n"
      ],
      "metadata": {
        "id": "jCfcjW5JWh7H"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot embeddings\n",
        "plot_embeddings(word2vec_model, 'Word2Vec_Embeddings')\n",
        "#plot_embeddings(glove_model, 'GloVe Embeddings')\n",
        "plot_embeddings(fasttext_model, 'FastText_Embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDKXE4j1WrRS",
        "outputId": "7bc16aef-f9bc-46e8-b8e4-db7bfd111294"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_of_vocabulary: 107\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 721 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 721 samples in 0.047s...\n",
            "[t-SNE] Computed conditional probabilities for sample 721 / 721\n",
            "[t-SNE] Mean sigma: 0.703712\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 64.031982\n",
            "[t-SNE] KL divergence after 20000 iterations: 0.177927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.fasttext:could not extract any ngrams from '', returning origin vector\n",
            "WARNING:gensim.models.fasttext:could not extract any ngrams from '', returning origin vector\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out_of_vocabulary: 0\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 828 samples in 0.001s...\n",
            "[t-SNE] Computed neighbors for 828 samples in 0.031s...\n",
            "[t-SNE] Computed conditional probabilities for sample 828 / 828\n",
            "[t-SNE] Mean sigma: 2.514999\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 64.555176\n",
            "[t-SNE] KL divergence after 20000 iterations: 0.678683\n"
          ]
        }
      ]
    }
  ]
}